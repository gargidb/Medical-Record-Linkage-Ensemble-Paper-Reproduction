{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FEBRL(Source-A)-code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Reproduction of results of Scheme A in original paper\n",
        "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
        "\n",
        "February 2019\n",
        "\n",
        "Jounal of Biomedical Informatics\n",
        "\n",
        "Source of original code provided by the author:\n",
        "\n",
        "Resources:\n",
        "\n",
        "  Ahmad Anis. Pytorch LSTM: The Definitive Guide. Mar. 2022. URL: https://cnvrg.io/pytorch-lstm/.\n",
        "\n",
        "  Improving LBFGS algorithm in pytorch. URL: http://sagecal.sourceforge.net/pytorch/index.html#: ̃:text=Closure,documentation%5C%2C%5C%20with%5C%20a%5C%20small%5C%20modification.\n",
        "\n",
        "  Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "\n",
        "Additional Experiments can be found towards the end of the notebook.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "kHvNBMqhsMCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-o_BNF26uZdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55eb74d-ca0f-41af-8762-20d109c1176a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the following files and save them in your local file system where they will be retrieved at the time of uploading: febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv from the repo provided by the original.\n",
        "Repo provided by authors can be found here: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble\n",
        "\n",
        "Upload febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv after running the following cell."
      ],
      "metadata": {
        "id": "PTUc5E85Yb2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zM2LqnUIuZs5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "fb1bc776-ca4e-4b2e-88ba-cd05ac81fa15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a5bb75e5-4ad6-48bf-a4dc-3b430495f324\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a5bb75e5-4ad6-48bf-a4dc-3b430495f324\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving febrl4_UNSW.csv to febrl4_UNSW.csv\n",
            "Saving ePBRN_D_dup.csv to ePBRN_D_dup.csv\n",
            "Saving ePBRN_F_dup.csv to ePBRN_F_dup.csv\n",
            "Saving febrl3_UNSW.csv to febrl3_UNSW.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recordlinkage \n",
        "import recordlinkage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeylTSEj01dH",
        "outputId": "93cbdf77-7f3d-47a2-dde8-bc35f024e3f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recordlinkage\n",
            "  Downloading recordlinkage-0.14-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.21.5)\n",
            "Collecting jellyfish>=0.5.4\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->recordlinkage) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->recordlinkage) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->recordlinkage) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->recordlinkage) (3.1.0)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73994 sha256=b75f5a600b3438ff6601210be02c349489a00d0f4b0aca636037cc59d1baab70\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish, recordlinkage\n",
            "Successfully installed jellyfish-0.9.0 recordlinkage-0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t5g9k8eOsQJq"
      },
      "outputs": [],
      "source": [
        "import recordlinkage as rl, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from recordlinkage.preprocessing import phonetic\n",
        "from numpy.random import choice\n",
        "import collections, numpy\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E1zWx4S8nZUk"
      },
      "outputs": [],
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "def generate_true_links(df): \n",
        "    # although the match_id column is included in the original df to imply the true links,\n",
        "    # this function will create the true_link object identical to the true_links properties\n",
        "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
        "    # in extract_function() for extracting features quicker.\n",
        "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
        "    df[\"rec_id\"] = df.index.values.tolist()\n",
        "    indices_1 = []\n",
        "    indices_2 = []\n",
        "    processed = 0\n",
        "    for match_id in df[\"match_id\"].unique():\n",
        "        if match_id != -1:    \n",
        "            processed = processed + 1\n",
        "            # print(\"In routine generate_true_links(), count =\", processed)\n",
        "            # clear_output(wait=True)\n",
        "            linkages = df.loc[df['match_id'] == match_id]\n",
        "            for j in range(len(linkages)-1):\n",
        "                for k in range(j+1, len(linkages)):\n",
        "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
        "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
        "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
        "    return links\n",
        "\n",
        "def generate_false_links(df, size):\n",
        "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
        "    # for training. The number of false pairs in specified as \"size\".\n",
        "    df[\"rec_id\"] = df.index.values.tolist()\n",
        "    indices_1 = []\n",
        "    indices_2 = []\n",
        "    unique_match_id = df[\"match_id\"].unique()\n",
        "    for j in range(size):\n",
        "            false_pair_ids = choice(unique_match_id, 2)\n",
        "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
        "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
        "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
        "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
        "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
        "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
        "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
        "    return links\n",
        "\n",
        "def swap_fields_flag(f11, f12, f21, f22):\n",
        "    return int((f11 == f22) and (f12 == f21))\n",
        "\n",
        "def extract_features(df, links):\n",
        "    c = rl.Compare()\n",
        "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name')\n",
        "    c.string('given_name_soundex', 'given_name_soundex', method='jarowinkler', label='y_name_soundex')\n",
        "    c.string('given_name_nysiis', 'given_name_nysiis', method='jarowinkler', label='y_name_nysiis')\n",
        "    c.string('surname', 'surname', method='jarowinkler', label='y_surname')\n",
        "    c.string('surname_soundex', 'surname_soundex', method='jarowinkler', label='y_surname_soundex')\n",
        "    c.string('surname_nysiis', 'surname_nysiis', method='jarowinkler', label='y_surname_nysiis')\n",
        "    c.exact('street_number', 'street_number', label='y_street_number')\n",
        "    c.string('address_1', 'address_1', method='levenshtein', threshold=0.7, label='y_address1')\n",
        "    c.string('address_2', 'address_2', method='levenshtein', threshold=0.7, label='y_address2')\n",
        "    c.exact('postcode', 'postcode', label='y_postcode')\n",
        "    c.exact('day', 'day', label='y_day')\n",
        "    c.exact('month', 'month', label='y_month')\n",
        "    c.exact('year', 'year', label='y_year')\n",
        "        \n",
        "    # Build features\n",
        "    feature_vectors = c.compute(links, df, df)\n",
        "    return feature_vectors\n",
        "\n",
        "def generate_train_X_y(df):\n",
        "    # This routine is to generate the feature vector X and the corresponding labels y\n",
        "    # with exactly equal number of samples for both classes to train the classifier.\n",
        "    pos = extract_features(df, train_true_links)\n",
        "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
        "    neg = extract_features(df, train_false_links)\n",
        "    X = pos.values.tolist() + neg.values.tolist()\n",
        "    y = [1]*len(pos)+[0]*len(neg)\n",
        "    X, y = shuffle(X, y, random_state=0)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
        "    if modeltype == 'svm': # Support Vector Machine\n",
        "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
        "        model.fit(train_vectors, train_labels) \n",
        "    elif modeltype == 'lg': # Logistic Regression\n",
        "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
        "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
        "                                   n_jobs=1, random_state=None)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    elif modeltype == 'nb': # Naive Bayes\n",
        "        model = GaussianNB()\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    elif modeltype == 'nn': # Neural Network\n",
        "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
        "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
        "                              learning_rate='constant',  learning_rate_init=0.001, \n",
        "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
        "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
        "                              nesterovs_momentum=True, early_stopping=False, \n",
        "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    ## addtion for testing ablation\n",
        "    elif modeltype == 'nn_ablation': # Neural Network\n",
        "        model = MLPClassifier(solver='sgd', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
        "                              activation = 'tanh',random_state=None, batch_size='auto', \n",
        "                              learning_rate='adaptive',  learning_rate_init=0.001, \n",
        "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
        "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
        "                              nesterovs_momentum=True, early_stopping=False, \n",
        "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    return model\n",
        "\n",
        "def classify(model, test_vectors):\n",
        "    result = model.predict(test_vectors)\n",
        "    return result\n",
        "\n",
        "    \n",
        "def evaluation(test_labels, result):\n",
        "    true_pos = np.logical_and(test_labels, result)\n",
        "    count_true_pos = np.sum(true_pos)\n",
        "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
        "    count_true_neg = np.sum(true_neg)\n",
        "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
        "    count_false_pos = np.sum(false_pos)\n",
        "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
        "    count_false_neg = np.sum(false_neg)\n",
        "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
        "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
        "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
        "    no_links_found = np.count_nonzero(result)\n",
        "    no_false = count_false_pos + count_false_neg\n",
        "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
        "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
        "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
        "    return metrics_result\n",
        "\n",
        "def blocking_performance(candidates, true_links, df):\n",
        "    count = 0\n",
        "    for candi in candidates:\n",
        "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
        "            count = count + 1\n",
        "    return count\n",
        "\n",
        "\n",
        "trainset = 'febrl3_UNSW'\n",
        "testset = 'febrl4_UNSW'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates the train dataset."
      ],
      "metadata": {
        "id": "9OQAIfR1ZTMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## TRAIN SET CONSTRUCTION\n",
        "\n",
        "# Import\n",
        "print(\"Import train set...\")\n",
        "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
        "train_true_links = generate_true_links(df_train)\n",
        "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
        "\n",
        "# Preprocess train set\n",
        "df_train['postcode'] = df_train['postcode'].astype(str)\n",
        "df_train['given_name_soundex'] = phonetic(df_train['given_name'], method='soundex')\n",
        "df_train['given_name_nysiis'] = phonetic(df_train['given_name'], method='nysiis')\n",
        "df_train['surname_soundex'] = phonetic(df_train['surname'], method='soundex')\n",
        "df_train['surname_nysiis'] = phonetic(df_train['surname'], method='nysiis')\n",
        "\n",
        "# Final train feature vectors and labels\n",
        "X_train, y_train = generate_train_X_y(df_train)\n",
        "print(\"Finished building X_train, y_train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH6sUwnE0-kj",
        "outputId": "33eaa64f-410f-4b56-a4e6-94ba5f242383"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import train set...\n",
            "Train set size: 5000 , number of matched pairs:  1165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished building X_train, y_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
        "# Import\n",
        "print(\"Import test set...\")\n",
        "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
        "test_true_links = generate_true_links(df_test)\n",
        "leng_test_true_links = len(test_true_links)\n",
        "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
        "\n",
        "print(\"BLOCKING PERFORMANCE:\")\n",
        "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
        "all_candidate_pairs = []\n",
        "for field in blocking_fields:\n",
        "    block_indexer = rl.BlockIndex(on=field)\n",
        "    candidates = block_indexer.index(df_test)\n",
        "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
        "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
        "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
        "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
        "          str(leng_test_true_links-detects) )\n",
        "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
        "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
        "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
        "          str(leng_test_true_links-detects) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7usR0bc1CE5",
        "outputId": "76565828-3508-49e3-ce2c-429af2bc9ddc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import test set...\n",
            "Test set size: 10000 , number of matched pairs:  5000\n",
            "BLOCKING PERFORMANCE:\n",
            "Number of pairs of matched given_name: 154898 , detected  3287 /5000 true matched pairs, missed 1713\n",
            "Number of pairs of matched surname: 170843 , detected  3325 /5000 true matched pairs, missed 1675\n",
            "Number of pairs of matched postcode: 53197 , detected  4219 /5000 true matched pairs, missed 781\n",
            "Number of pairs of at least 1 field matched: 372073 , detected  4894 /5000 true matched pairs, missed 106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a test dataset."
      ],
      "metadata": {
        "id": "Uluv3GAcZfB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## TEST SET CONSTRUCTION\n",
        "\n",
        "# Preprocess test set\n",
        "print(\"Processing test set...\")\n",
        "print(\"Preprocess...\")\n",
        "df_test['postcode'] = df_test['postcode'].astype(str)\n",
        "df_test['given_name_soundex'] = phonetic(df_test['given_name'], method='soundex')\n",
        "df_test['given_name_nysiis'] = phonetic(df_test['given_name'], method='nysiis')\n",
        "df_test['surname_soundex'] = phonetic(df_test['surname'], method='soundex')\n",
        "df_test['surname_nysiis'] = phonetic(df_test['surname'], method='nysiis')\n",
        "\n",
        "# Test feature vectors and labels construction\n",
        "print(\"Extract feature vectors...\")\n",
        "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
        "vectors = df_X_test.values.tolist()\n",
        "labels = [0]*len(vectors)\n",
        "feature_index = df_X_test.index\n",
        "for i in range(0, len(feature_index)):\n",
        "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
        "        labels[i] = 1\n",
        "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
        "print(\"Finished building X_test, y_test\")"
      ],
      "metadata": {
        "id": "3_QRC-Hz1HWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919563d3-2f44-48f9-8322-c7211ac3e75b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test set...\n",
            "Preprocess...\n",
            "Extract feature vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/recordlinkage/preprocessing/encoding.py:80: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  s = s.str.replace(r\"[\\-\\_\\s]\", \"\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count labels of y_test: Counter({0: 367179, 1: 4894})\n",
            "Finished building X_test, y_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline code for the SVM model."
      ],
      "metadata": {
        "id": "42yM8rb1Zi90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## SVM MODEL; linear kernel; C=0.005\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'svm' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'linear'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [.005] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHqPGtIg94zH",
        "outputId": "fba65e0e-4b26-4296-8259-404b9a35ce90"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: svm , Param_1: linear , tuning range: [0.005]\n",
            "Total training time 0.030590295791625977\n",
            "No_false: [81] \n",
            "\n",
            "Precision: [0.9872443814537356] \n",
            "\n",
            "Sensitivity: [0.9963220269718022] \n",
            "\n",
            "F-score: [0.9917624326248348] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline code for the NN model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kPTgXKzmZrZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## NN MODEL; relu\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'relu'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [100] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MwnFZyH-X31",
        "outputId": "79f8fd90-df48-4b80-8b22-9d4087024624"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: nn , Param_1: relu , tuning range: [100]\n",
            "Total training time 0.48044896125793457\n",
            "No_false: [79] \n",
            "\n",
            "Precision: [0.9896278218425869] \n",
            "\n",
            "Sensitivity: [0.9942787086228034] \n",
            "\n",
            "F-score: [0.991947813678524] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ablation of NN model:\n",
        "Changing to activation tp logistic and alpha parameter to 50"
      ],
      "metadata": {
        "id": "xiLH7ggzXX8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## NN logistic; C=50\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'logistic'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [50] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZSYnnj3E7Kc",
        "outputId": "19230d2b-9422-4979-b7e6-80d1523cefeb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: nn , Param_1: logistic , tuning range: [50]\n",
            "Total training time 4.082343816757202\n",
            "No_false: [81] \n",
            "\n",
            "Precision: [0.9912226985099] \n",
            "\n",
            "Sensitivity: [0.9922353902738047] \n",
            "\n",
            "F-score: [0.9917287858674565] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline for the LG model"
      ],
      "metadata": {
        "id": "Y3tJQyFHba6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## LG MODEL; l2; C=0.2\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'lg' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'l2'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [0.2] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTvP0w3c-_3t",
        "outputId": "758b392a-877a-4652-dfca-7ae8e7548093"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: lg , Param_1: l2 , tuning range: [0.2]\n",
            "Total training time 0.026948213577270508\n",
            "No_false: [144] \n",
            "\n",
            "Precision: [0.9746203037569944] \n",
            "\n",
            "Sensitivity: [0.996526358806702] \n",
            "\n",
            "F-score: [0.9854516063851283] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging performance for all baseline models."
      ],
      "metadata": {
        "id": "3kueCB0xgSwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
        "\n",
        "print(\"BAGGING PERFORMANCE:\\n\")\n",
        "modeltypes = ['svm', 'nn', 'lg'] \n",
        "modeltypes_2 = ['linear', 'relu', 'l2']\n",
        "modelparams = [0.005, 100, 0.2]\n",
        "nFold = 10\n",
        "kf = KFold(n_splits=nFold)\n",
        "model_raw_score = [0]*3\n",
        "model_binary_score = [0]*3\n",
        "model_i = 0\n",
        "for model_i in range(3):\n",
        "    start_bagging_time = time.time()\n",
        "    modeltype = modeltypes[model_i]\n",
        "    modeltype_2 = modeltypes_2[model_i]\n",
        "    modelparam = modelparams[model_i]\n",
        "    print(modeltype, \"per fold:\")\n",
        "    iFold = 0\n",
        "    result_fold = [0]*nFold\n",
        "    final_eval_fold = [0]*nFold\n",
        "    start_training_time = time.time()\n",
        "    for train_index, valid_index in kf.split(X_train):\n",
        "        X_train_fold = X_train[train_index]\n",
        "        y_train_fold = y_train[train_index]\n",
        "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
        "        result_fold[iFold] = classify(md, X_test)\n",
        "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
        "        print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
        "        iFold = iFold + 1\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    bagging_raw_score = np.average(result_fold, axis=0)\n",
        "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
        "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
        "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
        "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
        "    print(modeltype, \"bagging:\", bagging_eval)\n",
        "    end_bagging_time = time.time()\n",
        "    print ('Total bagging time', end_bagging_time-start_bagging_time)\n",
        "    print('')\n",
        "    model_raw_score[model_i] = bagging_raw_score\n",
        "    model_binary_score[model_i] = bagging_binary_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxDfbkl-_5jm",
        "outputId": "aedae88c-d4ed-457e-9d87-9d04a5b061f0"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAGGING PERFORMANCE:\n",
            "\n",
            "svm per fold:\n",
            "Fold 0 {'no_false': 80, 'confusion_matrix': [4876, 62, 18, 367117], 'precision': 0.987444309437019, 'sensitivity': 0.9963220269718022, 'no_links': 4938, 'F-score': 0.9918633034987795}\n",
            "Fold 1 {'no_false': 83, 'confusion_matrix': [4876, 65, 18, 367114], 'precision': 0.9868447682655332, 'sensitivity': 0.9963220269718022, 'no_links': 4941, 'F-score': 0.9915607524148449}\n",
            "Fold 2 {'no_false': 84, 'confusion_matrix': [4876, 66, 18, 367113], 'precision': 0.9866450829623634, 'sensitivity': 0.9963220269718022, 'no_links': 4942, 'F-score': 0.991459943066287}\n",
            "Fold 3 {'no_false': 80, 'confusion_matrix': [4876, 62, 18, 367117], 'precision': 0.987444309437019, 'sensitivity': 0.9963220269718022, 'no_links': 4938, 'F-score': 0.9918633034987795}\n",
            "Fold 4 {'no_false': 85, 'confusion_matrix': [4876, 67, 18, 367112], 'precision': 0.9864454784543799, 'sensitivity': 0.9963220269718022, 'no_links': 4943, 'F-score': 0.9913591542136829}\n",
            "Fold 5 {'no_false': 79, 'confusion_matrix': [4876, 61, 18, 367118], 'precision': 0.9876443184119911, 'sensitivity': 0.9963220269718022, 'no_links': 4937, 'F-score': 0.9919641948937036}\n",
            "Fold 6 {'no_false': 81, 'confusion_matrix': [4876, 63, 18, 367116], 'precision': 0.9872443814537356, 'sensitivity': 0.9963220269718022, 'no_links': 4939, 'F-score': 0.9917624326248348}\n",
            "Fold 7 {'no_false': 80, 'confusion_matrix': [4876, 62, 18, 367117], 'precision': 0.987444309437019, 'sensitivity': 0.9963220269718022, 'no_links': 4938, 'F-score': 0.9918633034987795}\n",
            "Fold 8 {'no_false': 80, 'confusion_matrix': [4876, 62, 18, 367117], 'precision': 0.987444309437019, 'sensitivity': 0.9963220269718022, 'no_links': 4938, 'F-score': 0.9918633034987795}\n",
            "Fold 9 {'no_false': 81, 'confusion_matrix': [4876, 63, 18, 367116], 'precision': 0.9872443814537356, 'sensitivity': 0.9963220269718022, 'no_links': 4939, 'F-score': 0.9917624326248348}\n",
            "Total training time 32.800180196762085\n",
            "svm bagging: {'no_false': 82, 'confusion_matrix': [4876, 64, 18, 367115], 'precision': 0.9870445344129555, 'sensitivity': 0.9963220269718022, 'no_links': 4940, 'F-score': 0.9916615822656092}\n",
            "Total bagging time 32.83165907859802\n",
            "\n",
            "nn per fold:\n",
            "Fold 0 {'no_false': 80, 'confusion_matrix': [4862, 48, 32, 367131], 'precision': 0.990224032586558, 'sensitivity': 0.9934613812832039, 'no_links': 4910, 'F-score': 0.9918400652794778}\n",
            "Fold 1 {'no_false': 73, 'confusion_matrix': [4864, 43, 30, 367136], 'precision': 0.9912370083554106, 'sensitivity': 0.9938700449530037, 'no_links': 4907, 'F-score': 0.9925517804305684}\n",
            "Fold 2 {'no_false': 79, 'confusion_matrix': [4865, 50, 29, 367129], 'precision': 0.9898270600203459, 'sensitivity': 0.9940743767879036, 'no_links': 4915, 'F-score': 0.9919461718829646}\n",
            "Fold 3 {'no_false': 82, 'confusion_matrix': [4862, 50, 32, 367129], 'precision': 0.9898208469055375, 'sensitivity': 0.9934613812832039, 'no_links': 4912, 'F-score': 0.991637772792168}\n",
            "Fold 4 {'no_false': 73, 'confusion_matrix': [4865, 44, 29, 367135], 'precision': 0.9910368710531676, 'sensitivity': 0.9940743767879036, 'no_links': 4909, 'F-score': 0.992553300010201}\n",
            "Fold 5 {'no_false': 81, 'confusion_matrix': [4865, 52, 29, 367127], 'precision': 0.9894244458002848, 'sensitivity': 0.9940743767879036, 'no_links': 4917, 'F-score': 0.9917439608602591}\n",
            "Fold 6 {'no_false': 79, 'confusion_matrix': [4865, 50, 29, 367129], 'precision': 0.9898270600203459, 'sensitivity': 0.9940743767879036, 'no_links': 4915, 'F-score': 0.9919461718829646}\n",
            "Fold 7 {'no_false': 82, 'confusion_matrix': [4862, 50, 32, 367129], 'precision': 0.9898208469055375, 'sensitivity': 0.9934613812832039, 'no_links': 4912, 'F-score': 0.991637772792168}\n",
            "Fold 8 {'no_false': 79, 'confusion_matrix': [4864, 49, 30, 367130], 'precision': 0.990026460411154, 'sensitivity': 0.9938700449530037, 'no_links': 4913, 'F-score': 0.9919445294177629}\n",
            "Fold 9 {'no_false': 80, 'confusion_matrix': [4865, 51, 29, 367128], 'precision': 0.9896257119609438, 'sensitivity': 0.9940743767879036, 'no_links': 4916, 'F-score': 0.9918450560652395}\n",
            "Total training time 11.33983325958252\n",
            "nn bagging: {'no_false': 78, 'confusion_matrix': [4865, 49, 29, 367130], 'precision': 0.99002849002849, 'sensitivity': 0.9940743767879036, 'no_links': 4914, 'F-score': 0.992047308319739}\n",
            "Total bagging time 11.375160217285156\n",
            "\n",
            "lg per fold:\n",
            "Fold 0 {'no_false': 128, 'confusion_matrix': [4875, 109, 19, 367070], 'precision': 0.9781300160513644, 'sensitivity': 0.9961176951369023, 'no_links': 4984, 'F-score': 0.9870419113180806}\n",
            "Fold 1 {'no_false': 116, 'confusion_matrix': [4876, 98, 18, 367081], 'precision': 0.9802975472456775, 'sensitivity': 0.9963220269718022, 'no_links': 4974, 'F-score': 0.9882448317794892}\n",
            "Fold 2 {'no_false': 143, 'confusion_matrix': [4878, 127, 16, 367052], 'precision': 0.9746253746253746, 'sensitivity': 0.9967306906416019, 'no_links': 5005, 'F-score': 0.9855540963733711}\n",
            "Fold 3 {'no_false': 143, 'confusion_matrix': [4876, 125, 18, 367054], 'precision': 0.9750049990002, 'sensitivity': 0.9963220269718022, 'no_links': 5001, 'F-score': 0.9855482566953007}\n",
            "Fold 4 {'no_false': 144, 'confusion_matrix': [4877, 127, 17, 367052], 'precision': 0.9746203037569944, 'sensitivity': 0.996526358806702, 'no_links': 5004, 'F-score': 0.9854516063851283}\n",
            "Fold 5 {'no_false': 145, 'confusion_matrix': [4877, 128, 17, 367051], 'precision': 0.9744255744255744, 'sensitivity': 0.996526358806702, 'no_links': 5005, 'F-score': 0.9853520557632084}\n",
            "Fold 6 {'no_false': 138, 'confusion_matrix': [4877, 121, 17, 367058], 'precision': 0.9757903161264506, 'sensitivity': 0.996526358806702, 'no_links': 4998, 'F-score': 0.9860493327941771}\n",
            "Fold 7 {'no_false': 135, 'confusion_matrix': [4876, 117, 18, 367062], 'precision': 0.9765671940717003, 'sensitivity': 0.9963220269718022, 'no_links': 4993, 'F-score': 0.9863457064832608}\n",
            "Fold 8 {'no_false': 156, 'confusion_matrix': [4874, 136, 20, 367043], 'precision': 0.9728542914171656, 'sensitivity': 0.9959133633020024, 'no_links': 5010, 'F-score': 0.9842487883683361}\n",
            "Fold 9 {'no_false': 143, 'confusion_matrix': [4876, 125, 18, 367054], 'precision': 0.9750049990002, 'sensitivity': 0.9963220269718022, 'no_links': 5001, 'F-score': 0.9855482566953007}\n",
            "Total training time 0.374248743057251\n",
            "lg bagging: {'no_false': 136, 'confusion_matrix': [4876, 118, 18, 367061], 'precision': 0.9763716459751702, 'sensitivity': 0.9963220269718022, 'no_links': 4994, 'F-score': 0.9862459546925566}\n",
            "Total bagging time 0.4171466827392578\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "thres = .99\n",
        "\n",
        "print(\"STACKING PERFORMANCE:\\n\")\n",
        "stack_raw_score = np.average(model_raw_score, axis=0)\n",
        "stack_binary_score = np.copy(stack_raw_score)\n",
        "stack_binary_score[stack_binary_score > thres] = 1\n",
        "stack_binary_score[stack_binary_score <= thres] = 0\n",
        "stacking_eval = evaluation(y_test, stack_binary_score)\n",
        "print(stacking_eval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXYmpqevAHUG",
        "outputId": "c03fb608-fa1d-4c04-9dc8-542b2a535258"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STACKING PERFORMANCE:\n",
            "\n",
            "{'no_false': 60, 'confusion_matrix': [4861, 27, 33, 367152], 'precision': 0.9944762684124386, 'sensitivity': 0.9932570494483041, 'no_links': 4888, 'F-score': 0.9938662850132898}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimenting NN results using PyTorch library instead of using Sklearn as by the original implementation."
      ],
      "metadata": {
        "id": "UR1W84GWBEr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "IFaJAKuuBqZf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is the experimentation of implementing an MLP classifier using the PyTorch library with 4 layers and relu activation."
      ],
      "metadata": {
        "id": "wL_hwKfebj3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "        def __init__(self, input_dim, hidden_size):\n",
        "            super(MLP, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_dim, 512)\n",
        "            self.fc2 = nn.Linear(512, 256)\n",
        "            self.fc3 = nn.Linear(256, 128)\n",
        "            self.fc4 = nn.Linear(128, 1)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            output = self.fc1(x)\n",
        "            output = F.relu(output)\n",
        "            output = self.fc2(output)\n",
        "            output = F.relu(output)\n",
        "            output = self.fc3(output)\n",
        "            output = F.relu(output)\n",
        "            output = self.fc4(output)\n",
        "            return output"
      ],
      "metadata": {
        "id": "CnJfG_jnM2Z4"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performace(y_test, result):\n",
        "    acc = accuracy_score(result, y_test)\n",
        "    p, r, f, _ = precision_recall_fscore_support(result, y_test, average='binary')\n",
        "    roc_auc = 0.\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(result, y_test)\n",
        "    except ValueError:\n",
        "        pass\n",
        "    # roc_auc = roc_auc_score(result, y_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, result).ravel()\n",
        "    no_links_found = np.count_nonzero(result)\n",
        "    no_false = fp + fn\n",
        "    cm = [tp, fp, fn, tn]\n",
        "    metrics_result = {'no_false':no_false, 'confusion_matrix':cm ,'precision':p,\n",
        "                        'sensitivity':r ,'no_links':no_links_found, 'F-score': f, 'roc_auc': roc_auc, 'accuracy':acc}\n",
        "    return metrics_result"
      ],
      "metadata": {
        "id": "J8bW8RUTB_2Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch = torch.from_numpy(X_train)\n",
        "y_batch = torch.from_numpy(y_train)\n",
        "\n",
        "batch_size=200\n",
        "\n",
        "my_dataset = torch.utils.data.TensorDataset(x_batch,y_batch) \n",
        "train_loader = torch.utils.data.DataLoader(dataset=my_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Ob12tWprCCbL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Model"
      ],
      "metadata": {
        "id": "VdO9nXJWb29J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MLP model\n",
        "input_dim=x_batch.size()[1]\n",
        "hidden_size=256\n",
        "num_epoch=100\n",
        "model = MLP(input_dim, hidden_size)#.to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fct = nn.MSELoss().cuda()\n",
        "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.001, tolerance_grad=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWu7E-SnCIt_",
        "outputId": "3036048f-b269-469c-c190-d9a97d066228"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=13, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP training with 100 epochs"
      ],
      "metadata": {
        "id": "HyyqoA4ub6bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sources used: Improving LBFGS algorithm in pytorch. URL: http://sagecal.sourceforge.net/pytorch/index.html#: ̃:text=Closure,documentation%5C%2C%5C%20with%5C%20a%5C%20small%5C%20modification.\n",
        "####\n",
        "total_training_time = time.time()\n",
        "times = np.array([])\n",
        "#train MLP\n",
        "for epoch in range(num_epoch):\n",
        "        startTime = time.time()\n",
        "        losses_arr = np.array([])\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            px, py = data.float(), target.float()\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                output = model(px)\n",
        "                loss = loss_fct(output, py.unsqueeze(1))\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            l = optimizer.step(closure)\n",
        "            losses_arr = np.append(losses_arr, l.item())\n",
        "        avg_loss = np.average(losses_arr)\n",
        "        print('Epoch {}: avg train loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "        executionTime = (time.time() - startTime)\n",
        "        times = np.append(times, executionTime)\n",
        "\n",
        "end_training_time = (time.time() - total_training_time)\n",
        "print ('Total training time', end_training_time)\n",
        "avg_runtime_for_each_epoch = np.average(times)\n",
        "print ('Average runtime for each epoch for MLP : ', avg_runtime_for_each_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OqasMghDc8a",
        "outputId": "5ef13a39-a408-487d-c3be-a647ea50a100"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: avg train loss: 0.06833279454572634\n",
            "Epoch 1: avg train loss: 0.02622341076758775\n",
            "Epoch 2: avg train loss: 0.018823575821112503\n",
            "Epoch 3: avg train loss: 0.01761209617622874\n",
            "Epoch 4: avg train loss: 0.017771575938571583\n",
            "Epoch 5: avg train loss: 0.02987327900799838\n",
            "Epoch 6: avg train loss: 0.023504851385951042\n",
            "Epoch 7: avg train loss: 0.014021376486529003\n",
            "Epoch 8: avg train loss: 0.012935584411025047\n",
            "Epoch 9: avg train loss: 0.013305147347802465\n",
            "Epoch 10: avg train loss: 0.013357972810891542\n",
            "Epoch 11: avg train loss: 0.01331077329814434\n",
            "Epoch 12: avg train loss: 0.013449008407240564\n",
            "Epoch 13: avg train loss: 0.01365531354465268\n",
            "Epoch 14: avg train loss: 0.013441263698041439\n",
            "Epoch 15: avg train loss: 0.013355729254809294\n",
            "Epoch 16: avg train loss: 0.013490675440566107\n",
            "Epoch 17: avg train loss: 0.013007214123552496\n",
            "Epoch 18: avg train loss: 0.013563064180991867\n",
            "Epoch 19: avg train loss: 0.01351778827268969\n",
            "Epoch 20: avg train loss: 0.01351767194203355\n",
            "Epoch 21: avg train loss: 0.013423666019331325\n",
            "Epoch 22: avg train loss: 0.013276883654973724\n",
            "Epoch 23: avg train loss: 0.013396679931743578\n",
            "Epoch 24: avg train loss: 0.013186775994571772\n",
            "Epoch 25: avg train loss: 0.013524964028461412\n",
            "Epoch 26: avg train loss: 0.013332340950992975\n",
            "Epoch 27: avg train loss: 0.013372777199203318\n",
            "Epoch 28: avg train loss: 0.013205022889782082\n",
            "Epoch 29: avg train loss: 0.013148400085893545\n",
            "Epoch 30: avg train loss: 0.013602519069205631\n",
            "Epoch 31: avg train loss: 0.013334754261780869\n",
            "Epoch 32: avg train loss: 0.01327892215075818\n",
            "Epoch 33: avg train loss: 0.013275445015593008\n",
            "Epoch 34: avg train loss: 0.012852632779289375\n",
            "Epoch 35: avg train loss: 0.013404115611179308\n",
            "Epoch 36: avg train loss: 0.013123866424641827\n",
            "Epoch 37: avg train loss: 0.013511818494986404\n",
            "Epoch 38: avg train loss: 0.013443142091008749\n",
            "Epoch 39: avg train loss: 0.013443226333368908\n",
            "Epoch 40: avg train loss: 0.013402841477231546\n",
            "Epoch 41: avg train loss: 0.013373561203479767\n",
            "Epoch 42: avg train loss: 0.013426392000507225\n",
            "Epoch 43: avg train loss: 0.013503393581645056\n",
            "Epoch 44: avg train loss: 0.013600655154071072\n",
            "Epoch 45: avg train loss: 0.013255727816034447\n",
            "Epoch 46: avg train loss: 0.013368550095368515\n",
            "Epoch 47: avg train loss: 0.013568366708403284\n",
            "Epoch 48: avg train loss: 0.013341378420591354\n",
            "Epoch 49: avg train loss: 0.013355618173425848\n",
            "Epoch 50: avg train loss: 0.013457049446349794\n",
            "Epoch 51: avg train loss: 0.013515613973140717\n",
            "Epoch 52: avg train loss: 0.013179261322048578\n",
            "Epoch 53: avg train loss: 0.01342876526442441\n",
            "Epoch 54: avg train loss: 0.013458971780809488\n",
            "Epoch 55: avg train loss: 0.013268333351747557\n",
            "Epoch 56: avg train loss: 0.013057682155208155\n",
            "Epoch 57: avg train loss: 0.013498702425171028\n",
            "Epoch 58: avg train loss: 0.013238895853812044\n",
            "Epoch 59: avg train loss: 0.013349330476061865\n",
            "Epoch 60: avg train loss: 0.013024226000363176\n",
            "Epoch 61: avg train loss: 0.013254190371795134\n",
            "Epoch 62: avg train loss: 0.013392869129099629\n",
            "Epoch 63: avg train loss: 0.013375591486692429\n",
            "Epoch 64: avg train loss: 0.013479298488660292\n",
            "Epoch 65: avg train loss: 0.013264311900870367\n",
            "Epoch 66: avg train loss: 0.013520629822530529\n",
            "Epoch 67: avg train loss: 0.01346144105561755\n",
            "Epoch 68: avg train loss: 0.013574845157563686\n",
            "Epoch 69: avg train loss: 0.013148111629892479\n",
            "Epoch 70: avg train loss: 0.013529602861539885\n",
            "Epoch 71: avg train loss: 0.013240100561895153\n",
            "Epoch 72: avg train loss: 0.013553380204195326\n",
            "Epoch 73: avg train loss: 0.013628606430508873\n",
            "Epoch 74: avg train loss: 0.01343654002994299\n",
            "Epoch 75: avg train loss: 0.01320206340063702\n",
            "Epoch 76: avg train loss: 0.013565056957304478\n",
            "Epoch 77: avg train loss: 0.01366321344605901\n",
            "Epoch 78: avg train loss: 0.013363647867332806\n",
            "Epoch 79: avg train loss: 0.013263497162948956\n",
            "Epoch 80: avg train loss: 0.012873278761451895\n",
            "Epoch 81: avg train loss: 0.013493477028201927\n",
            "Epoch 82: avg train loss: 0.013568681410767815\n",
            "Epoch 83: avg train loss: 0.013435924425721169\n",
            "Epoch 84: avg train loss: 0.013540698046034033\n",
            "Epoch 85: avg train loss: 0.013206220655278727\n",
            "Epoch 86: avg train loss: 0.013061514208939943\n",
            "Epoch 87: avg train loss: 0.013087738474661654\n",
            "Epoch 88: avg train loss: 0.013408991338854486\n",
            "Epoch 89: avg train loss: 0.013334510085934942\n",
            "Epoch 90: avg train loss: 0.013309775343672796\n",
            "Epoch 91: avg train loss: 0.013595047914846376\n",
            "Epoch 92: avg train loss: 0.013215611096132885\n",
            "Epoch 93: avg train loss: 0.013259235430847515\n",
            "Epoch 94: avg train loss: 0.013438960706645792\n",
            "Epoch 95: avg train loss: 0.01351844807240096\n",
            "Epoch 96: avg train loss: 0.012919885106384754\n",
            "Epoch 97: avg train loss: 0.013510724529623985\n",
            "Epoch 98: avg train loss: 0.013599383306096901\n",
            "Epoch 99: avg train loss: 0.013570436869155277\n",
            "Total training time 181.09244346618652\n",
            "Average runtime for each epoch for MLP :  1.8106678867340087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch_test = torch.from_numpy(X_test)#.cuda()\n",
        "y_batch_test = torch.from_numpy(y_test)#.cuda()"
      ],
      "metadata": {
        "id": "KafU9JaSE62t"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using test data on the MLP classifier implemented in PyTorch."
      ],
      "metadata": {
        "id": "QUhN3IW9cDDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model test\n",
        "y_pred = model(x_batch_test.float())\n",
        "after_train = loss_fct(y_pred.squeeze(), y_batch_test.float()) \n",
        "print('Test loss after Training' , after_train.item())\n",
        "y_hat = (y_pred > 0.5).int()#.cpu()\n",
        "stacking_eval = evaluate_performace(y_test, y_hat)\n",
        "print(stacking_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrSmOah3E-bq",
        "outputId": "427726a9-3564-42ba-bc30-0e9a40b04d5e"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss after Training 0.02214452251791954\n",
            "{'no_false': 131, 'confusion_matrix': [4870, 107, 24, 367072], 'precision': 0.9950960359624029, 'sensitivity': 0.9785011050833836, 'no_links': 4977, 'F-score': 0.9867288015398643, 'roc_auc': 0.9892178635448081, 'accuracy': 0.9996479185536171}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LSTM Model - Experiment"
      ],
      "metadata": {
        "id": "bwgxKQueIbCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sources used: Ahmad Anis. Pytorch LSTM: The Definitive Guide. Mar. 2022. URL: https://cnvrg.io/pytorch-lstm/.\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_size, num_layers, output_size, batch_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        num_samples = x.size(0)\n",
        "        ht = torch.zeros(self.num_layers, num_samples, self.hidden_size)\n",
        "        ct = torch.zeros(self.num_layers, num_samples, self.hidden_size)\n",
        "        l1, l2 = [l for l in (ht,ct)]\n",
        "        output, _ = self.lstm(x, (l1, l2))\n",
        "        output = output[:,-1,:]\n",
        "        output = self.fc1(output)\n",
        "        output = self.sigmoid(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "kaVH8GPnIiKU"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0005\n",
        "n_epochs = 10\n",
        "input_dim = x_batch.size()[1]    \n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "x_batch = torch.from_numpy(X_train)\n",
        "y_batch = torch.from_numpy(y_train)\n",
        "\n",
        "x_test_batch = torch.from_numpy(X_test)\n",
        "y_test_batch = torch.from_numpy(y_test)\n",
        "\n",
        "my_dataset = torch.utils.data.TensorDataset(x_batch,y_batch)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=my_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n"
      ],
      "metadata": {
        "id": "u-JygJqQIlc_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM model"
      ],
      "metadata": {
        "id": "vToE_gmfcSbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(input_dim, hidden_size, num_layers, output_size, batch_size)\n",
        "print(model)\n",
        "criterion = nn.BCELoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVLAn-KMIwdK",
        "outputId": "5cda5f8f-2aae-451e-9cce-7339ed36482d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(13, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM model implemented using PyTorch training using 10 epochs."
      ],
      "metadata": {
        "id": "eMFmOIUIcUGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Start model training')\n",
        "start_training_time = time.time()\n",
        "epoch_times = np.array([])\n",
        "for epoch in range(0, n_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    losses = np.array([])\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "        x_batch = x_batch.reshape(batch_size,  1, input_dim)\n",
        "  \n",
        "        opt.zero_grad()\n",
        "        out = model(x_batch.float())\n",
        "\n",
        "        loss = criterion(out, y_batch.float().unsqueeze(1))\n",
        "        losses = np.append(losses, loss.item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_times = np.append(epoch_times, epoch_end_time-epoch_start_time)\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, np.average(losses)))\n",
        "\n",
        "end_training_time = time.time()\n",
        "print ('Average epoch training time', np.average(epoch_times))\n",
        "print ('Total training time', end_training_time-start_training_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQYq8iFI3mY",
        "outputId": "2932502e-154d-4e3d-d95b-eea9c5656e07"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start model training\n",
            "Epoch 0: train loss: 0.581181164417002\n",
            "Epoch 1: train loss: 0.17439208536719283\n",
            "Epoch 2: train loss: 0.031143200732508883\n",
            "Epoch 3: train loss: 0.015829219890292734\n",
            "Epoch 4: train loss: 0.011918853138922714\n",
            "Epoch 5: train loss: 0.01119055348681286\n",
            "Epoch 6: train loss: 0.010492084235819574\n",
            "Epoch 7: train loss: 0.009968274703876685\n",
            "Epoch 8: train loss: 0.00975786931667244\n",
            "Epoch 9: train loss: 0.00958296051572284\n",
            "Average epoch training time 0.3683640003204346\n",
            "Total training time 3.6905150413513184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch_test = torch.from_numpy(X_test)\n",
        "y_batch_test = torch.from_numpy(y_test)\n",
        "my_dataset = torch.utils.data.TensorDataset(x_batch_test,y_batch_test) \n",
        "test_loader = torch.utils.data.DataLoader(dataset=my_dataset, batch_size=batch_size, drop_last=True, shuffle=True)"
      ],
      "metadata": {
        "id": "YaKxr6I_MPxp"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate LSTM model on test data."
      ],
      "metadata": {
        "id": "K-7xDh8accdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def eval_model(model, val_loader):\n",
        "    model.eval()\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, (x_batch, y_batch) in enumerate(test_loader):\n",
        "          x_batch = x_batch.reshape(batch_size,  1, input_dim)\n",
        "          y_hat = model(x_batch.float())\n",
        "          y_hat = (y_hat > 0.5).int()\n",
        "          Y_pred.extend(y_hat.tolist())\n",
        "          Y_true.extend(y_batch.tolist())\n",
        "\n",
        "    return Y_pred, Y_true\n",
        "\n",
        "y_pred, y_true = eval_model(model, test_loader)\n",
        "res = evaluate_performace(y_true, y_pred)\n",
        "print (res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIdZ3UMZKIpF",
        "outputId": "2a243b5e-7806-4d63-9767-42c1d633d4dd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'no_false': 980, 'confusion_matrix': [4884, 970, 10, 366200], 'precision': 0.9979566816510013, 'sensitivity': 0.8343013324222753, 'no_links': 5854, 'F-score': 0.9088202456270933, 'roc_auc': 0.9171370128428518, 'accuracy': 0.997366044551475}\n"
          ]
        }
      ]
    }
  ]
}