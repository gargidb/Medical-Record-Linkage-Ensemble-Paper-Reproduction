{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ePBRN(Source-B)-code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Reproduction of results of Scheme B in original paper\n",
        "Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "Kha Vo, Jitendra Jonnagaddala, Siaw-Teng Liaw\n",
        "\n",
        "February 2019\n",
        "\n",
        "Jounal of Biomedical Informatics\n",
        "\n",
        "Source of original code provided by the author:\n",
        "\n",
        "Resources:\n",
        "\n",
        "Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "\n",
        "Additional Experiments can be found towards the end of the notebook.\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kHvNBMqhsMCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-o_BNF26uZdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0474fc8c-227b-4133-db79-62f4514678d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the following files and save them in your local file system where they will be retrieved at the time of uploading: febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv from the repo provided by the original.\n",
        "Repo provided by authors can be found here: https://github.com/ePBRN/Medical-Record-Linkage-Ensemble\n",
        "\n",
        "Upload febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv after running the following cell."
      ],
      "metadata": {
        "id": "cKhceXy8eXj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload febrl4_UNSW.csv, ePBRN_D_dup.csv, ePBRN_F_dup.csv, febrl3_UNSW.csv\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "zM2LqnUIuZs5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "4a415ac8-6ce4-4210-a3cb-f07efa92e7b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fca29ca7-24a3-4bfe-b820-d170da54ea70\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fca29ca7-24a3-4bfe-b820-d170da54ea70\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving febrl4_UNSW.csv to febrl4_UNSW.csv\n",
            "Saving ePBRN_D_dup.csv to ePBRN_D_dup.csv\n",
            "Saving ePBRN_F_dup.csv to ePBRN_F_dup.csv\n",
            "Saving febrl3_UNSW.csv to febrl3_UNSW.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install recordlinkage \n",
        "import recordlinkage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeylTSEj01dH",
        "outputId": "6fba71ad-eafd-4a0b-c70d-8655df537e85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recordlinkage\n",
            "  Downloading recordlinkage-0.14-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 22.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.21.5)\n",
            "Collecting jellyfish>=0.5.4\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.3.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from recordlinkage) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->recordlinkage) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->recordlinkage) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->recordlinkage) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->recordlinkage) (3.1.0)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=73968 sha256=0320a7a1024adfa35566168b693a2c48bbb1bc524128c32c4afe6cd061741a86\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish, recordlinkage\n",
            "Successfully installed jellyfish-0.9.0 recordlinkage-0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t5g9k8eOsQJq"
      },
      "outputs": [],
      "source": [
        "import recordlinkage as rl, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from recordlinkage.preprocessing import phonetic\n",
        "from numpy.random import choice\n",
        "import collections, numpy\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E1zWx4S8nZUk"
      },
      "outputs": [],
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "trainset = 'ePBRN_F_dup' \n",
        "testset = 'ePBRN_D_dup'\n",
        "\n",
        "import recordlinkage as rl, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from recordlinkage.preprocessing import phonetic\n",
        "from numpy.random import choice\n",
        "import collections, numpy\n",
        "from IPython.display import clear_output\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "\n",
        "def generate_true_links(df): \n",
        "    # although the match_id column is included in the original df to imply the true links,\n",
        "    # this function will create the true_link object identical to the true_links properties\n",
        "    # of recordlinkage toolkit, in order to exploit \"Compare.compute()\" from that toolkit\n",
        "    # in extract_function() for extracting features quicker.\n",
        "    # This process should be deprecated in the future release of the UNSW toolkit.\n",
        "    df[\"rec_id\"] = df.index.values.tolist()\n",
        "    indices_1 = []\n",
        "    indices_2 = []\n",
        "    processed = 0\n",
        "    for match_id in df[\"match_id\"].unique():\n",
        "        if match_id != -1:    \n",
        "            processed = processed + 1\n",
        "            # print(\"In routine generate_true_links(), count =\", processed)\n",
        "            # clear_output(wait=True)\n",
        "            linkages = df.loc[df['match_id'] == match_id]\n",
        "            for j in range(len(linkages)-1):\n",
        "                for k in range(j+1, len(linkages)):\n",
        "                    indices_1 = indices_1 + [linkages.iloc[j][\"rec_id\"]]\n",
        "                    indices_2 = indices_2 + [linkages.iloc[k][\"rec_id\"]]    \n",
        "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
        "    return links\n",
        "\n",
        "def generate_false_links(df, size):\n",
        "    # A counterpart of generate_true_links(), with the purpose to generate random false pairs\n",
        "    # for training. The number of false pairs in specified as \"size\".\n",
        "    df[\"rec_id\"] = df.index.values.tolist()\n",
        "    indices_1 = []\n",
        "    indices_2 = []\n",
        "    unique_match_id = df[\"match_id\"].unique()\n",
        "    unique_match_id = unique_match_id[~np.isnan(unique_match_id)] # remove nan values\n",
        "    for j in range(size):\n",
        "            false_pair_ids = choice(unique_match_id, 2)\n",
        "            candidate_1_cluster = df.loc[df['match_id'] == false_pair_ids[0]]\n",
        "            candidate_1 = candidate_1_cluster.iloc[choice(range(len(candidate_1_cluster)))]\n",
        "            candidate_2_cluster = df.loc[df['match_id'] == false_pair_ids[1]]\n",
        "            candidate_2 = candidate_2_cluster.iloc[choice(range(len(candidate_2_cluster)))]    \n",
        "            indices_1 = indices_1 + [candidate_1[\"rec_id\"]]\n",
        "            indices_2 = indices_2 + [candidate_2[\"rec_id\"]]  \n",
        "    links = pd.MultiIndex.from_arrays([indices_1,indices_2])\n",
        "    return links\n",
        "\n",
        "def swap_fields_flag(f11, f12, f21, f22):\n",
        "    return ((f11 == f22) & (f12 == f21)).astype(float)\n",
        "\n",
        "def join_names_space(f11, f12, f21, f22):\n",
        "    return ((f11+\" \"+f12 == f21) | (f11+\" \"+f12 == f22)| (f21+\" \"+f22 == f11)| (f21+\" \"+f22 == f12)).astype(float)\n",
        "\n",
        "def join_names_dash(f11, f12, f21, f22):\n",
        "    return ((f11+\"-\"+f12 == f21) | (f11+\"-\"+f12 == f22)| (f21+\"-\"+f22 == f11)| (f21+\"-\"+f22 == f12)).astype(float)\n",
        "\n",
        "def abb_surname(f1, f2):\n",
        "    return ((f1[0]==f2) | (f1==f2[0])).astype(float)\n",
        "\n",
        "def reset_day(f11, f12, f21, f22):\n",
        "    return (((f11 == 1) & (f12 == 1))|((f21 == 1) & (f22 == 1))).astype(float)\n",
        "\n",
        "def extract_features(df, links):\n",
        "    c = rl.Compare()\n",
        "    c.string('given_name', 'given_name', method='levenshtein', label='y_name_leven')\n",
        "    c.string('surname', 'surname', method='levenshtein', label='y_surname_leven')  \n",
        "    c.string('given_name', 'given_name', method='jarowinkler', label='y_name_jaro')\n",
        "    c.string('surname', 'surname', method='jarowinkler', label='y_surname_jaro')  \n",
        "    c.string('postcode', 'postcode', method='jarowinkler', label='y_postcode')      \n",
        "    exact_fields = ['postcode', 'address_1', 'address_2', 'street_number']\n",
        "    for field in exact_fields:\n",
        "        c.exact(field, field, label='y_'+field+'_exact')\n",
        "    c.compare_vectorized(reset_day,('day', 'month'), ('day', 'month'),label='reset_day_flag')    \n",
        "    c.compare_vectorized(swap_fields_flag,('day', 'month'), ('day', 'month'),label='swap_day_month')    \n",
        "    c.compare_vectorized(swap_fields_flag,('surname', 'given_name'), ('surname', 'given_name'),label='swap_names')    \n",
        "    c.compare_vectorized(join_names_space,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_space')\n",
        "    c.compare_vectorized(join_names_dash,('surname', 'given_name'), ('surname', 'given_name'),label='join_names_dash')\n",
        "    c.compare_vectorized(abb_surname,'surname', 'surname',label='abb_surname')\n",
        "    # Build features\n",
        "    feature_vectors = c.compute(links, df, df)\n",
        "    return feature_vectors\n",
        "\n",
        "def generate_train_X_y(df):\n",
        "    # This routine is to generate the feature vector X and the corresponding labels y\n",
        "    # with exactly equal number of samples for both classes to train the classifier.\n",
        "    pos = extract_features(df, train_true_links)\n",
        "    train_false_links = generate_false_links(df, len(train_true_links))    \n",
        "    neg = extract_features(df, train_false_links)\n",
        "    X = pos.values.tolist() + neg.values.tolist()\n",
        "    y = [1]*len(pos)+[0]*len(neg)\n",
        "    X, y = shuffle(X, y, random_state=0)\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y\n",
        "\n",
        "def train_model(modeltype, modelparam, train_vectors, train_labels, modeltype_2):\n",
        "    if modeltype == 'svm': # Support Vector Machine\n",
        "        model = svm.SVC(C = modelparam, kernel = modeltype_2)\n",
        "        model.fit(train_vectors, train_labels) \n",
        "    elif modeltype == 'lg': # Logistic Regression\n",
        "        model = LogisticRegression(C=modelparam, penalty = modeltype_2,class_weight=None, dual=False, fit_intercept=True, \n",
        "                                   intercept_scaling=1, max_iter=5000, multi_class='ovr', \n",
        "                                   n_jobs=1, random_state=None)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    elif modeltype == 'nb': # Naive Bayes\n",
        "        model = GaussianNB()\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    elif modeltype == 'nn': # Neural Network\n",
        "        model = MLPClassifier(solver='lbfgs', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
        "                              activation = modeltype_2,random_state=None, batch_size='auto', \n",
        "                              learning_rate='constant',  learning_rate_init=0.001, \n",
        "                              power_t=0.5, max_iter=30000, shuffle=True, \n",
        "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
        "                              nesterovs_momentum=True, early_stopping=False, \n",
        "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    elif modeltype == 'nn_ablation': # Neural Network\n",
        "        model = MLPClassifier(solver='sgd', alpha=modelparam, hidden_layer_sizes=(256, ), \n",
        "                              activation = 'tanh',random_state=None, batch_size='auto', \n",
        "                              learning_rate='adaptive',  learning_rate_init=0.001, \n",
        "                              power_t=0.5, max_iter=10000, shuffle=True, \n",
        "                              tol=0.0001, verbose=False, warm_start=False, momentum=0.9, \n",
        "                              nesterovs_momentum=True, early_stopping=False, \n",
        "                              validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        model.fit(train_vectors, train_labels)\n",
        "    return model\n",
        "\n",
        "def classify(model, test_vectors):\n",
        "    result = model.predict(test_vectors)\n",
        "    return result\n",
        "\n",
        "    \n",
        "def evaluation(test_labels, result):\n",
        "    true_pos = np.logical_and(test_labels, result)\n",
        "    count_true_pos = np.sum(true_pos)\n",
        "    true_neg = np.logical_and(np.logical_not(test_labels),np.logical_not(result))\n",
        "    count_true_neg = np.sum(true_neg)\n",
        "    false_pos = np.logical_and(np.logical_not(test_labels), result)\n",
        "    count_false_pos = np.sum(false_pos)\n",
        "    false_neg = np.logical_and(test_labels,np.logical_not(result))\n",
        "    count_false_neg = np.sum(false_neg)\n",
        "    precision = count_true_pos/(count_true_pos+count_false_pos)\n",
        "    sensitivity = count_true_pos/(count_true_pos+count_false_neg) # sensitivity = recall\n",
        "    confusion_matrix = [count_true_pos, count_false_pos, count_false_neg, count_true_neg]\n",
        "    no_links_found = np.count_nonzero(result)\n",
        "    no_false = count_false_pos + count_false_neg\n",
        "    Fscore = 2*precision*sensitivity/(precision+sensitivity)\n",
        "    metrics_result = {'no_false':no_false, 'confusion_matrix':confusion_matrix ,'precision':precision,\n",
        "                     'sensitivity':sensitivity ,'no_links':no_links_found, 'F-score': Fscore}\n",
        "    return metrics_result\n",
        "\n",
        "def blocking_performance(candidates, true_links, df):\n",
        "    count = 0\n",
        "    for candi in candidates:\n",
        "        if df.loc[candi[0]][\"match_id\"]==df.loc[candi[1]][\"match_id\"]:\n",
        "            count = count + 1\n",
        "    return count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates the train dataset."
      ],
      "metadata": {
        "id": "wGK5DbDyejpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## TRAIN SET CONSTRUCTION\n",
        "\n",
        "# Import\n",
        "print(\"Import train set...\")\n",
        "df_train = pd.read_csv(trainset+\".csv\", index_col = \"rec_id\")\n",
        "train_true_links = generate_true_links(df_train)\n",
        "print(\"Train set size:\", len(df_train), \", number of matched pairs: \", str(len(train_true_links)))\n",
        "\n",
        "# Preprocess train set\n",
        "df_train['postcode'] = df_train['postcode'].astype(str)\n",
        "\n",
        "# Final train feature vectors and labels\n",
        "X_train, y_train = generate_train_X_y(df_train)\n",
        "print(\"Finished building X_train, y_train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH6sUwnE0-kj",
        "outputId": "ae134322-ee6c-4ff6-9ce8-d3c5017a145f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import train set...\n",
            "Train set size: 14078 , number of matched pairs:  3192\n",
            "Finished building X_train, y_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "# Blocking Criteria: declare non-match of all of the below fields disagree\n",
        "# Import\n",
        "print(\"Import test set...\")\n",
        "df_test = pd.read_csv(testset+\".csv\", index_col = \"rec_id\")\n",
        "test_true_links = generate_true_links(df_test)\n",
        "leng_test_true_links = len(test_true_links)\n",
        "print(\"Test set size:\", len(df_test), \", number of matched pairs: \", str(leng_test_true_links))\n",
        "\n",
        "print(\"BLOCKING PERFORMANCE:\")\n",
        "blocking_fields = [\"given_name\", \"surname\", \"postcode\"]\n",
        "all_candidate_pairs = []\n",
        "for field in blocking_fields:\n",
        "    block_indexer = rl.BlockIndex(on=field)\n",
        "    candidates = block_indexer.index(df_test)\n",
        "    detects = blocking_performance(candidates, test_true_links, df_test)\n",
        "    all_candidate_pairs = candidates.union(all_candidate_pairs)\n",
        "    print(\"Number of pairs of matched \"+ field +\": \"+str(len(candidates)), \", detected \",\n",
        "         detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
        "          str(leng_test_true_links-detects) )\n",
        "detects = blocking_performance(all_candidate_pairs, test_true_links, df_test)\n",
        "print(\"Number of pairs of at least 1 field matched: \" + str(len(all_candidate_pairs)), \", detected \",\n",
        "     detects,'/'+ str(leng_test_true_links) + \" true matched pairs, missed \" + \n",
        "          str(leng_test_true_links-detects) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7usR0bc1CE5",
        "outputId": "570e8f8d-f67b-42c6-b32d-3a30f0324561"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import test set...\n",
            "Test set size: 11731 , number of matched pairs:  2653\n",
            "BLOCKING PERFORMANCE:\n",
            "Number of pairs of matched given_name: 252552 , detected  1567 /2653 true matched pairs, missed 1086\n",
            "Number of pairs of matched surname: 33832 , detected  1480 /2653 true matched pairs, missed 1173\n",
            "Number of pairs of matched postcode: 79940 , detected  2462 /2653 true matched pairs, missed 191\n",
            "Number of pairs of at least 1 field matched: 362910 , detected  2599 /2653 true matched pairs, missed 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a test dataset."
      ],
      "metadata": {
        "id": "U0_oyP2zenXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## TEST SET CONSTRUCTION\n",
        "\n",
        "# Preprocess test set\n",
        "print(\"Processing test set...\")\n",
        "print(\"Preprocess...\")\n",
        "df_test['postcode'] = df_test['postcode'].astype(str)\n",
        "\n",
        "# Test feature vectors and labels construction\n",
        "print(\"Extract feature vectors...\")\n",
        "df_X_test = extract_features(df_test, all_candidate_pairs)\n",
        "vectors = df_X_test.values.tolist()\n",
        "labels = [0]*len(vectors)\n",
        "feature_index = df_X_test.index\n",
        "for i in range(0, len(feature_index)):\n",
        "    if df_test.loc[feature_index[i][0]][\"match_id\"]==df_test.loc[feature_index[i][1]][\"match_id\"]:\n",
        "        labels[i] = 1\n",
        "X_test, y_test = shuffle(vectors, labels, random_state=0)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "print(\"Count labels of y_test:\",collections.Counter(y_test))\n",
        "print(\"Finished building X_test, y_test\")"
      ],
      "metadata": {
        "id": "3_QRC-Hz1HWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e392c13-6cb2-4038-b84f-1fb454b7b5a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test set...\n",
            "Preprocess...\n",
            "Extract feature vectors...\n",
            "Count labels of y_test: Counter({0: 360311, 1: 2599})\n",
            "Finished building X_test, y_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline code for the SVM model."
      ],
      "metadata": {
        "id": "Stg76Rmce1sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## SVM MODEL; rbf; C=0.001\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'svm' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'rbf'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [.001] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHqPGtIg94zH",
        "outputId": "7f5401d7-ee6c-4681-dffc-934e001f82d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: svm , Param_1: rbf , tuning range: [0.001]\n",
            "Total training time 2.0498197078704834\n",
            "No_false: [5537] \n",
            "\n",
            "Precision: [0.3178323412698413] \n",
            "\n",
            "Sensitivity: [0.9861485186610235] \n",
            "\n",
            "F-score: [0.4807277501641189] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline code for the NN model."
      ],
      "metadata": {
        "id": "JboTtHV7e8zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## NN MODEL; relu; C=2000\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'nn' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'relu'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [2000] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MwnFZyH-X31",
        "outputId": "870067d3-359f-4c4c-f9cd-ec934caed76a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: nn , Param_1: relu , tuning range: [2000]\n",
            "Total training time 0.9440796375274658\n",
            "No_false: [1208] \n",
            "\n",
            "Precision: [0.6919679823350814] \n",
            "\n",
            "Sensitivity: [0.9646017699115044] \n",
            "\n",
            "F-score: [0.8058502089360334] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell is the baseline code for the LG model."
      ],
      "metadata": {
        "id": "573U8VJxgK4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## BASE LEARNERS CLASSIFICATION AND EVALUATION\n",
        "## LG MODEL; l2; C=0.005\n",
        "print(\"BASE LEARNERS CLASSIFICATION PERFORMANCE:\")\n",
        "modeltype = 'lg' # choose between 'svm', 'lg', 'nn'\n",
        "modeltype_2 = 'l2'#'rbf'  # 'linear' or 'rbf' for svm, 'l1' or 'l2' for lg, 'relu' or 'logistic' for nn\n",
        "modelparam_range = [0.005] # C for svm, C for lg, alpha for NN\n",
        "print(\"Model:\",modeltype,\", Param_1:\",modeltype_2, \", tuning range:\", modelparam_range)\n",
        "precision = []\n",
        "sensitivity = []\n",
        "Fscore = []\n",
        "nb_false = []\n",
        "\n",
        "for modelparam in modelparam_range:\n",
        "    start_training_time = time.time()\n",
        "    md = train_model(modeltype, modelparam, X_train, y_train, modeltype_2)\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    final_result = classify(md, X_test)\n",
        "    final_eval = evaluation(y_test, final_result)\n",
        "    precision += [final_eval['precision']]\n",
        "    sensitivity += [final_eval['sensitivity']]\n",
        "    Fscore += [final_eval['F-score']]\n",
        "    nb_false  += [final_eval['no_false']]\n",
        "    \n",
        "print(\"No_false:\",nb_false,\"\\n\")\n",
        "print(\"Precision:\",precision,\"\\n\")\n",
        "print(\"Sensitivity:\",sensitivity,\"\\n\")\n",
        "print(\"F-score:\", Fscore,\"\\n\")\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTvP0w3c-_3t",
        "outputId": "bb812b46-4254-4b16-e8e8-e46ad6eb8516"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE LEARNERS CLASSIFICATION PERFORMANCE:\n",
            "Model: lg , Param_1: l2 , tuning range: [0.005]\n",
            "Total training time 0.023532390594482422\n",
            "No_false: [1827] \n",
            "\n",
            "Precision: [0.5905678085405913] \n",
            "\n",
            "Sensitivity: [0.9684494036167757] \n",
            "\n",
            "F-score: [0.7337122868386532] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging performance for all baseline models."
      ],
      "metadata": {
        "id": "KV5eMkJsgXv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "## ENSEMBLE CLASSIFICATION AND EVALUATION\n",
        "\n",
        "print(\"BAGGING PERFORMANCE:\\n\")\n",
        "modeltypes = ['svm', 'nn', 'lg'] \n",
        "modeltypes_2 = ['rbf', 'relu', 'l2']\n",
        "modelparams = [0.001, 2000, 0.005]\n",
        "nFold = 10\n",
        "kf = KFold(n_splits=nFold)\n",
        "model_raw_score = [0]*3\n",
        "model_binary_score = [0]*3\n",
        "model_i = 0\n",
        "for model_i in range(3):\n",
        "    start_bagging_time = time.time()\n",
        "    modeltype = modeltypes[model_i]\n",
        "    modeltype_2 = modeltypes_2[model_i]\n",
        "    modelparam = modelparams[model_i]\n",
        "    print(modeltype, \"per fold:\")\n",
        "    iFold = 0\n",
        "    result_fold = [0]*nFold\n",
        "    final_eval_fold = [0]*nFold\n",
        "    start_training_time = time.time()\n",
        "    for train_index, valid_index in kf.split(X_train):\n",
        "        X_train_fold = X_train[train_index]\n",
        "        y_train_fold = y_train[train_index]\n",
        "        md =  train_model(modeltype, modelparam, X_train_fold, y_train_fold, modeltype_2)\n",
        "        result_fold[iFold] = classify(md, X_test)\n",
        "        final_eval_fold[iFold] = evaluation(y_test, result_fold[iFold])\n",
        "        print(\"Fold\", str(iFold), final_eval_fold[iFold])\n",
        "        iFold = iFold + 1\n",
        "    end_training_time = time.time()\n",
        "    print ('Total training time', end_training_time-start_training_time)\n",
        "    bagging_raw_score = np.average(result_fold, axis=0)\n",
        "    bagging_binary_score  = np.copy(bagging_raw_score)\n",
        "    bagging_binary_score[bagging_binary_score > 0.5] = 1\n",
        "    bagging_binary_score[bagging_binary_score <= 0.5] = 0\n",
        "    bagging_eval = evaluation(y_test, bagging_binary_score)\n",
        "    print(modeltype, \"bagging:\", bagging_eval)\n",
        "    end_bagging_time = time.time()\n",
        "    print ('Total bagging time', end_bagging_time-start_bagging_time)\n",
        "    print('')\n",
        "    model_raw_score[model_i] = bagging_raw_score\n",
        "    model_binary_score[model_i] = bagging_binary_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxDfbkl-_5jm",
        "outputId": "108438c7-a6fb-4cad-f676-0d0d4afd5605"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAGGING PERFORMANCE:\n",
            "\n",
            "svm per fold:\n",
            "Fold 0 {'no_false': 4581, 'confusion_matrix': [2550, 4532, 49, 355779], 'precision': 0.3600677774639932, 'sensitivity': 0.9811465948441709, 'no_links': 7082, 'F-score': 0.5268050821196157}\n",
            "Fold 1 {'no_false': 4545, 'confusion_matrix': [2551, 4497, 48, 355814], 'precision': 0.3619466515323496, 'sensitivity': 0.981531358214698, 'no_links': 7048, 'F-score': 0.5288690784699906}\n",
            "Fold 2 {'no_false': 4660, 'confusion_matrix': [2552, 4613, 47, 355698], 'precision': 0.3561758548499651, 'sensitivity': 0.9819161215852251, 'no_links': 7165, 'F-score': 0.5227365833674724}\n",
            "Fold 3 {'no_false': 4614, 'confusion_matrix': [2551, 4566, 48, 355745], 'precision': 0.3584375439089504, 'sensitivity': 0.981531358214698, 'no_links': 7117, 'F-score': 0.5251132153149444}\n",
            "Fold 4 {'no_false': 4563, 'confusion_matrix': [2550, 4514, 49, 355797], 'precision': 0.36098527746319364, 'sensitivity': 0.9811465948441709, 'no_links': 7064, 'F-score': 0.5277864017385905}\n",
            "Fold 5 {'no_false': 4563, 'confusion_matrix': [2551, 4515, 48, 355796], 'precision': 0.3610246249646193, 'sensitivity': 0.981531358214698, 'no_links': 7066, 'F-score': 0.5278841179513709}\n",
            "Fold 6 {'no_false': 4601, 'confusion_matrix': [2550, 4552, 49, 355759], 'precision': 0.3590537876654464, 'sensitivity': 0.9811465948441709, 'no_links': 7102, 'F-score': 0.5257189980414391}\n",
            "Fold 7 {'no_false': 4587, 'confusion_matrix': [2550, 4538, 49, 355773], 'precision': 0.3597629796839729, 'sensitivity': 0.9811465948441709, 'no_links': 7088, 'F-score': 0.5264787860018582}\n",
            "Fold 8 {'no_false': 4621, 'confusion_matrix': [2550, 4572, 49, 355739], 'precision': 0.35804549283909015, 'sensitivity': 0.9811465948441709, 'no_links': 7122, 'F-score': 0.5246373829852896}\n",
            "Fold 9 {'no_false': 4550, 'confusion_matrix': [2550, 4501, 49, 355810], 'precision': 0.3616508296695504, 'sensitivity': 0.9811465948441709, 'no_links': 7051, 'F-score': 0.5284974093264249}\n",
            "Total training time 1079.2805109024048\n",
            "svm bagging: {'no_false': 4571, 'confusion_matrix': [2550, 4522, 49, 355789], 'precision': 0.3605769230769231, 'sensitivity': 0.9811465948441709, 'no_links': 7072, 'F-score': 0.527349808706442}\n",
            "Total bagging time 1079.3103342056274\n",
            "\n",
            "nn per fold:\n",
            "Fold 0 {'no_false': 1234, 'confusion_matrix': [2508, 1143, 91, 359168], 'precision': 0.6869350862777321, 'sensitivity': 0.9649865332820315, 'no_links': 3651, 'F-score': 0.80256}\n",
            "Fold 1 {'no_false': 1081, 'confusion_matrix': [2506, 988, 93, 359323], 'precision': 0.7172295363480252, 'sensitivity': 0.9642170065409773, 'no_links': 3494, 'F-score': 0.8225832923026425}\n",
            "Fold 2 {'no_false': 1263, 'confusion_matrix': [2510, 1174, 89, 359137], 'precision': 0.6813246471226927, 'sensitivity': 0.9657560600230858, 'no_links': 3684, 'F-score': 0.7989813783224574}\n",
            "Fold 3 {'no_false': 1057, 'confusion_matrix': [2506, 964, 93, 359347], 'precision': 0.7221902017291066, 'sensitivity': 0.9642170065409773, 'no_links': 3470, 'F-score': 0.8258362168396771}\n",
            "Fold 4 {'no_false': 1123, 'confusion_matrix': [2507, 1031, 92, 359280], 'precision': 0.708592425098926, 'sensitivity': 0.9646017699115044, 'no_links': 3538, 'F-score': 0.8170115691706046}\n",
            "Fold 5 {'no_false': 1039, 'confusion_matrix': [2506, 946, 93, 359365], 'precision': 0.7259559675550405, 'sensitivity': 0.9642170065409773, 'no_links': 3452, 'F-score': 0.8282928441579904}\n",
            "Fold 6 {'no_false': 1150, 'confusion_matrix': [2507, 1058, 92, 359253], 'precision': 0.7032258064516129, 'sensitivity': 0.9646017699115044, 'no_links': 3565, 'F-score': 0.8134328358208955}\n",
            "Fold 7 {'no_false': 1328, 'confusion_matrix': [2510, 1239, 89, 359072], 'precision': 0.6695118698319552, 'sensitivity': 0.9657560600230858, 'no_links': 3749, 'F-score': 0.7908002520478892}\n",
            "Fold 8 {'no_false': 1200, 'confusion_matrix': [2508, 1109, 91, 359202], 'precision': 0.6933923140724357, 'sensitivity': 0.9649865332820315, 'no_links': 3617, 'F-score': 0.8069498069498069}\n",
            "Fold 9 {'no_false': 1192, 'confusion_matrix': [2508, 1101, 91, 359210], 'precision': 0.6949293433083957, 'sensitivity': 0.9649865332820315, 'no_links': 3609, 'F-score': 0.8079896907216496}\n",
            "Total training time 14.68755316734314\n",
            "nn bagging: {'no_false': 1140, 'confusion_matrix': [2507, 1048, 92, 359263], 'precision': 0.7052039381153306, 'sensitivity': 0.9646017699115044, 'no_links': 3555, 'F-score': 0.8147546311342216}\n",
            "Total bagging time 14.719378232955933\n",
            "\n",
            "lg per fold:\n",
            "Fold 0 {'no_false': 1747, 'confusion_matrix': [2512, 1660, 87, 358651], 'precision': 0.6021093000958773, 'sensitivity': 0.96652558676414, 'no_links': 4172, 'F-score': 0.7419878895288731}\n",
            "Fold 1 {'no_false': 1795, 'confusion_matrix': [2512, 1708, 87, 358603], 'precision': 0.595260663507109, 'sensitivity': 0.96652558676414, 'no_links': 4220, 'F-score': 0.7367649215427482}\n",
            "Fold 2 {'no_false': 1798, 'confusion_matrix': [2517, 1716, 82, 358595], 'precision': 0.5946137491141035, 'sensitivity': 0.9684494036167757, 'no_links': 4233, 'F-score': 0.7368266978922717}\n",
            "Fold 3 {'no_false': 1765, 'confusion_matrix': [2518, 1684, 81, 358627], 'precision': 0.5992384578772013, 'sensitivity': 0.9688341669873028, 'no_links': 4202, 'F-score': 0.7404793412733423}\n",
            "Fold 4 {'no_false': 1767, 'confusion_matrix': [2511, 1679, 88, 358632], 'precision': 0.5992840095465394, 'sensitivity': 0.966140823393613, 'no_links': 4190, 'F-score': 0.7397260273972603}\n",
            "Fold 5 {'no_false': 1746, 'confusion_matrix': [2511, 1658, 88, 358653], 'precision': 0.60230271048213, 'sensitivity': 0.966140823393613, 'no_links': 4169, 'F-score': 0.7420212765957448}\n",
            "Fold 6 {'no_false': 1791, 'confusion_matrix': [2517, 1709, 82, 358602], 'precision': 0.5955986748698533, 'sensitivity': 0.9684494036167757, 'no_links': 4226, 'F-score': 0.7375824175824176}\n",
            "Fold 7 {'no_false': 1803, 'confusion_matrix': [2518, 1722, 81, 358589], 'precision': 0.5938679245283018, 'sensitivity': 0.9688341669873028, 'no_links': 4240, 'F-score': 0.7363649656382512}\n",
            "Fold 8 {'no_false': 1737, 'confusion_matrix': [2512, 1650, 87, 358661], 'precision': 0.6035559827006247, 'sensitivity': 0.96652558676414, 'no_links': 4162, 'F-score': 0.7430853424049696}\n",
            "Fold 9 {'no_false': 1785, 'confusion_matrix': [2512, 1698, 87, 358613], 'precision': 0.5966745843230403, 'sensitivity': 0.96652558676414, 'no_links': 4210, 'F-score': 0.7378469672492289}\n",
            "Total training time 0.42165637016296387\n",
            "lg bagging: {'no_false': 1773, 'confusion_matrix': [2512, 1686, 87, 358625], 'precision': 0.5983801810385898, 'sensitivity': 0.96652558676414, 'no_links': 4198, 'F-score': 0.7391496248344859}\n",
            "Total bagging time 0.4542539119720459\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source used: Kha Vo and Jitendra Jonnagaddala and Siaw-Teng Liaw. (2019). Medical-Record-Linkage-Ensemble. Retrieved from https://github.com/ePBRN/Medical-Record-Linkage-Ensemble. Paper: \"Statistical supervised meta-ensemble algorithm for data linkage\"\n",
        "\n",
        "thres = .99\n",
        "\n",
        "print(\"STACKING PERFORMANCE:\\n\")\n",
        "stack_raw_score = np.average(model_raw_score, axis=0)\n",
        "stack_binary_score = np.copy(stack_raw_score)\n",
        "stack_binary_score[stack_binary_score > thres] = 1\n",
        "stack_binary_score[stack_binary_score <= thres] = 0\n",
        "stacking_eval = evaluation(y_test, stack_binary_score)\n",
        "print(stacking_eval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXYmpqevAHUG",
        "outputId": "62459146-0399-4a99-f7ea-ac9afe00d39e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STACKING PERFORMANCE:\n",
            "\n",
            "{'no_false': 1036, 'confusion_matrix': [2505, 942, 94, 359369], 'precision': 0.7267188859878155, 'sensitivity': 0.9638322431704501, 'no_links': 3447, 'F-score': 0.8286470393648694}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is the experimentation of implementing an MLP classifier using the PyTorch library with 4 layers and relu activation."
      ],
      "metadata": {
        "id": "UR1W84GWBEr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "IFaJAKuuBqZf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "        def __init__(self, input_dim, hidden_size):\n",
        "            super(MLP, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_dim, 256)\n",
        "            self.fc2 = nn.Linear(256, 128)\n",
        "            self.fc3 = nn.Linear(128, 1)\n",
        "            \n",
        "        def forward(self, x):\n",
        "            output = self.fc1(x)\n",
        "            output = F.relu(output)\n",
        "            output = self.fc2(output)\n",
        "            output = F.relu(output)\n",
        "            output = self.fc3(output)\n",
        "            return output"
      ],
      "metadata": {
        "id": "1ifJ6VA1bKNG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performace(y_test, result):\n",
        "    acc = accuracy_score(result, y_test)\n",
        "    p, r, f, _ = precision_recall_fscore_support(result, y_test, average='binary')\n",
        "    roc_auc = 0.\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(result, y_test)\n",
        "    except ValueError:\n",
        "        pass\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, result).ravel()\n",
        "    no_links_found = np.count_nonzero(result)\n",
        "    no_false = fp + fn\n",
        "    cm = [tp, fp, fn, tn]\n",
        "    metrics_result = {'no_false':no_false, 'confusion_matrix':cm ,'precision':p,\n",
        "                        'sensitivity':r ,'no_links':no_links_found, 'F-score': f, 'roc_auc': roc_auc, 'accuracy':acc}\n",
        "    return metrics_result"
      ],
      "metadata": {
        "id": "J8bW8RUTB_2Q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch = torch.from_numpy(X_train)\n",
        "y_batch = torch.from_numpy(y_train)\n",
        "\n",
        "batch_size=200\n",
        "\n",
        "my_dataset = torch.utils.data.TensorDataset(x_batch,y_batch) \n",
        "train_loader = torch.utils.data.DataLoader(dataset=my_dataset, batch_size=batch_size, drop_last=True, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Ob12tWprCCbL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Model"
      ],
      "metadata": {
        "id": "06HSt0AegsU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MLP model\n",
        "input_dim=x_batch.size()[1]\n",
        "hidden_size=256\n",
        "num_epoch=100\n",
        "model = MLP(input_dim, hidden_size)#.to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fct = nn.MSELoss().cuda()\n",
        "optimizer = torch.optim.LBFGS(model.parameters(), lr=0.001, tolerance_grad=0.0001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWu7E-SnCIt_",
        "outputId": "810f0885-c48e-4314-c58f-c1ca36d3393d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=15, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "total_training_time = time.time()\n",
        "times = np.array([])\n",
        "#train MLP\n",
        "for epoch in range(num_epoch):\n",
        "        startTime = time.time()\n",
        "        losses_arr = np.array([])\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            px, py = data.float(), target.float()\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                output = model(px)\n",
        "                loss = loss_fct(output, py.unsqueeze(1))\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            l = optimizer.step(closure)\n",
        "            losses_arr = np.append(losses_arr, l.item())\n",
        "        avg_loss = np.average(losses_arr)\n",
        "        print('Epoch {}: avg train loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "        executionTime = (time.time() - startTime)\n",
        "        times = np.append(times, executionTime)\n",
        "\n",
        "end_training_time = (time.time() - total_training_time)\n",
        "print ('Total training time', end_training_time)\n",
        "avg_runtime_for_each_epoch = np.average(times)\n",
        "print ('Average runtime for each epoch for MLP : ', avg_runtime_for_each_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OqasMghDc8a",
        "outputId": "fb3a4dd0-0452-46d6-b131-48c9dda26132"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: avg train loss: 0.0900833565861948\n",
            "Epoch 1: avg train loss: 0.018745142065228954\n",
            "Epoch 2: avg train loss: 0.01736730301091748\n",
            "Epoch 3: avg train loss: 0.017382582228991292\n",
            "Epoch 4: avg train loss: 0.0174736640746555\n",
            "Epoch 5: avg train loss: 0.017216297076834787\n",
            "Epoch 6: avg train loss: 0.017155839580922358\n",
            "Epoch 7: avg train loss: 0.017333178089991692\n",
            "Epoch 8: avg train loss: 0.017318070355442264\n",
            "Epoch 9: avg train loss: 0.017262197850692655\n",
            "Epoch 10: avg train loss: 0.017141237374274962\n",
            "Epoch 11: avg train loss: 0.01726074648960944\n",
            "Epoch 12: avg train loss: 0.017359069218077967\n",
            "Epoch 13: avg train loss: 0.017411279582208204\n",
            "Epoch 14: avg train loss: 0.01735428056769794\n",
            "Epoch 15: avg train loss: 0.017247081794325384\n",
            "Epoch 16: avg train loss: 0.017376036383211613\n",
            "Epoch 17: avg train loss: 0.017392932497445616\n",
            "Epoch 18: avg train loss: 0.017388079045039993\n",
            "Epoch 19: avg train loss: 0.017181594405443437\n",
            "Epoch 20: avg train loss: 0.017239679281990373\n",
            "Epoch 21: avg train loss: 0.017239573501771497\n",
            "Epoch 22: avg train loss: 0.0173153571724411\n",
            "Epoch 23: avg train loss: 0.017393474076544086\n",
            "Epoch 24: avg train loss: 0.01738515559343561\n",
            "Epoch 25: avg train loss: 0.01721962105723158\n",
            "Epoch 26: avg train loss: 0.01742290011456897\n",
            "Epoch 27: avg train loss: 0.017545853140613726\n",
            "Epoch 28: avg train loss: 0.017321118273802342\n",
            "Epoch 29: avg train loss: 0.01733823156645221\n",
            "Epoch 30: avg train loss: 0.017228738855450385\n",
            "Epoch 31: avg train loss: 0.01741894598930113\n",
            "Epoch 32: avg train loss: 0.017351038964285005\n",
            "Epoch 33: avg train loss: 0.01713681497400807\n",
            "Epoch 34: avg train loss: 0.01717455453810192\n",
            "Epoch 35: avg train loss: 0.017247592519608237\n",
            "Epoch 36: avg train loss: 0.017340525293782834\n",
            "Epoch 37: avg train loss: 0.017187644727528095\n",
            "Epoch 38: avg train loss: 0.017442294096033418\n",
            "Epoch 39: avg train loss: 0.017314776087239865\n",
            "Epoch 40: avg train loss: 0.01749926781462085\n",
            "Epoch 41: avg train loss: 0.017564906618527828\n",
            "Epoch 42: avg train loss: 0.01733887493009529\n",
            "Epoch 43: avg train loss: 0.017480526600153215\n",
            "Epoch 44: avg train loss: 0.017417870972666047\n",
            "Epoch 45: avg train loss: 0.017218367827515447\n",
            "Epoch 46: avg train loss: 0.017292620103445747\n",
            "Epoch 47: avg train loss: 0.017326529888856794\n",
            "Epoch 48: avg train loss: 0.017308254455847126\n",
            "Epoch 49: avg train loss: 0.01703763092237134\n",
            "Epoch 50: avg train loss: 0.01727214356463763\n",
            "Epoch 51: avg train loss: 0.017239103634511272\n",
            "Epoch 52: avg train loss: 0.01724763496989204\n",
            "Epoch 53: avg train loss: 0.01731127593666315\n",
            "Epoch 54: avg train loss: 0.0172589676034066\n",
            "Epoch 55: avg train loss: 0.017416269127880375\n",
            "Epoch 56: avg train loss: 0.017320053411587592\n",
            "Epoch 57: avg train loss: 0.0171825400884113\n",
            "Epoch 58: avg train loss: 0.017326916417767926\n",
            "Epoch 59: avg train loss: 0.017356672714794835\n",
            "Epoch 60: avg train loss: 0.01727683238324619\n",
            "Epoch 61: avg train loss: 0.017489280070989363\n",
            "Epoch 62: avg train loss: 0.017233467630801663\n",
            "Epoch 63: avg train loss: 0.017356122333195903\n",
            "Epoch 64: avg train loss: 0.01719151404235632\n",
            "Epoch 65: avg train loss: 0.017355076112453977\n",
            "Epoch 66: avg train loss: 0.017431741700537743\n",
            "Epoch 67: avg train loss: 0.017130982971960498\n",
            "Epoch 68: avg train loss: 0.01741964117653908\n",
            "Epoch 69: avg train loss: 0.017280583360022115\n",
            "Epoch 70: avg train loss: 0.017140644392178904\n",
            "Epoch 71: avg train loss: 0.017433288447078196\n",
            "Epoch 72: avg train loss: 0.017294616258192445\n",
            "Epoch 73: avg train loss: 0.017418024941317497\n",
            "Epoch 74: avg train loss: 0.017346281978872516\n",
            "Epoch 75: avg train loss: 0.017119818547319983\n",
            "Epoch 76: avg train loss: 0.017435452360058983\n",
            "Epoch 77: avg train loss: 0.017366043982967254\n",
            "Epoch 78: avg train loss: 0.017338890492195082\n",
            "Epoch 79: avg train loss: 0.01730043894701427\n",
            "Epoch 80: avg train loss: 0.017444215084035552\n",
            "Epoch 81: avg train loss: 0.017330521597496925\n",
            "Epoch 82: avg train loss: 0.017314261486453396\n",
            "Epoch 83: avg train loss: 0.017552975985792377\n",
            "Epoch 84: avg train loss: 0.01710809902438233\n",
            "Epoch 85: avg train loss: 0.01742116998760931\n",
            "Epoch 86: avg train loss: 0.01735911368122024\n",
            "Epoch 87: avg train loss: 0.017287564283657457\n",
            "Epoch 88: avg train loss: 0.017100188745966843\n",
            "Epoch 89: avg train loss: 0.017249205059582187\n",
            "Epoch 90: avg train loss: 0.01739437923195862\n",
            "Epoch 91: avg train loss: 0.01755813701498893\n",
            "Epoch 92: avg train loss: 0.01744934090323025\n",
            "Epoch 93: avg train loss: 0.01707754927056451\n",
            "Epoch 94: avg train loss: 0.017406898640817212\n",
            "Epoch 95: avg train loss: 0.017336007988741322\n",
            "Epoch 96: avg train loss: 0.01719103453140105\n",
            "Epoch 97: avg train loss: 0.017438326902206865\n",
            "Epoch 98: avg train loss: 0.017500358333270395\n",
            "Epoch 99: avg train loss: 0.017512643108925512\n",
            "Total training time 72.73663067817688\n",
            "Average runtime for each epoch for MLP :  0.727117965221405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch_test = torch.from_numpy(X_test)#.cuda()\n",
        "y_batch_test = torch.from_numpy(y_test)#.cuda()"
      ],
      "metadata": {
        "id": "KafU9JaSE62t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Model"
      ],
      "metadata": {
        "id": "6zi0jh7Qh49l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model\n",
        "y_pred = model(x_batch_test.float())\n",
        "after_train = loss_fct(y_pred.squeeze(), y_batch_test.float()) \n",
        "print('Test loss after Training' , after_train.item())\n",
        "y_hat = (y_pred > 0.5).int()#.cpu()\n",
        "stacking_eval = evaluate_performace(y_test, y_hat)\n",
        "print(stacking_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrSmOah3E-bq",
        "outputId": "e6298168-4b6c-4c01-ed2e-96d4d305d769"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss after Training 0.05276618152856827\n",
            "{'no_false': 1952, 'confusion_matrix': [2522, 1875, 77, 358436], 'precision': 0.9703732204694113, 'sensitivity': 0.5735728906072322, 'no_links': 4397, 'F-score': 0.7209834190966266, 'roc_auc': 0.786679057287003, 'accuracy': 0.9946212559587777}\n"
          ]
        }
      ]
    }
  ]
}